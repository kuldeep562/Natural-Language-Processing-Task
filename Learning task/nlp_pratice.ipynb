{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e81e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd , matplotlib.pyplot as plt , numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19bdfc81-7dcc-49b4-8b6d-734aa65c57c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29ed7d2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer , SnowballStemmer , WordNetLemmatizer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, prefer_gpu, require_cpu, require_gpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattributeruler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttributeRuler\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdep_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DependencyParser\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01medit_tree_lemmatizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EditTreeLemmatizer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\attributeruler.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Language\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Matcher\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\language.py:43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_EXCEPTIONS, URL_MATCH\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlookups\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_lookups\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipe_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze_pipes, print_pipe_analysis, validate_attrs\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschemas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     45\u001b[0m     ConfigSchema,\n\u001b[0;32m     46\u001b[0m     ConfigSchemaInit,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     validate_init_settings,\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipe_analysis.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m msg\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc, Span, Token\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dot_to_dict\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# This lets us add type hints for mypy etc. without causing circular imports\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\tokens\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_serialize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocBin\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmorphanalysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MorphAnalysis\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\tokens\\_serialize.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleFrozenList, ensure_path\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocab\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dict_proxies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpanGroups\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DOCBIN_ALL_ATTRS \u001b[38;5;28;01mas\u001b[39;00m ALL_ATTRS\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\vocab.pyx:1\u001b[0m, in \u001b[0;36minit spacy.vocab\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:49\u001b[0m, in \u001b[0;36minit spacy.tokens.doc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\schemas.py:195\u001b[0m\n\u001b[0;32m    191\u001b[0m         obj \u001b[38;5;241m=\u001b[39m converted\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate(TokenPatternSchema, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpattern\u001b[39m\u001b[38;5;124m\"\u001b[39m: obj})\n\u001b[1;32m--> 195\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTokenPatternString\u001b[39;00m(BaseModel):\n\u001b[0;32m    196\u001b[0m     REGEX: Optional[Union[StrictStr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenPatternString\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m Field(\u001b[38;5;28;01mNone\u001b[39;00m, alias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    197\u001b[0m     IN: Optional[List[StrictStr]] \u001b[38;5;241m=\u001b[39m Field(\u001b[38;5;28;01mNone\u001b[39;00m, alias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\main.py:286\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[1;34m(mcs, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__signature__ \u001b[38;5;241m=\u001b[39m ClassAttribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__signature__\u001b[39m\u001b[38;5;124m'\u001b[39m, generate_model_signature(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m, fields, config))\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolve_forward_refs:\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__try_update_forward_refs__()\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# preserve `__set_name__` protocol defined in https://peps.python.org/pep-0487\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# for attributes not in `new_namespace` (e.g. private attributes)\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, obj \u001b[38;5;129;01min\u001b[39;00m namespace\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\main.py:808\u001b[0m, in \u001b[0;36mBaseModel.__try_update_forward_refs__\u001b[1;34m(cls, **localns)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__try_update_forward_refs__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlocalns: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    804\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;124;03m    Same as update_forward_refs but will not raise exception\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;124;03m    when forward references are not defined.\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 808\u001b[0m     update_model_forward_refs(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__fields__\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__config__\u001b[38;5;241m.\u001b[39mjson_encoders, localns, (\u001b[38;5;167;01mNameError\u001b[39;00m,))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\typing.py:554\u001b[0m, in \u001b[0;36mupdate_model_forward_refs\u001b[1;34m(model, fields, json_encoders, localns, exc_to_suppress)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields:\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 554\u001b[0m         update_field_forward_refs(f, globalns\u001b[38;5;241m=\u001b[39mglobalns, localns\u001b[38;5;241m=\u001b[39mlocalns)\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exc_to_suppress:\n\u001b[0;32m    556\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\typing.py:529\u001b[0m, in \u001b[0;36mupdate_field_forward_refs\u001b[1;34m(field, globalns, localns)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39msub_fields:\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sub_f \u001b[38;5;129;01min\u001b[39;00m field\u001b[38;5;241m.\u001b[39msub_fields:\n\u001b[1;32m--> 529\u001b[0m         update_field_forward_refs(sub_f, globalns\u001b[38;5;241m=\u001b[39mglobalns, localns\u001b[38;5;241m=\u001b[39mlocalns)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mdiscriminator_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     field\u001b[38;5;241m.\u001b[39mprepare_discriminated_union_sub_fields()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\typing.py:520\u001b[0m, in \u001b[0;36mupdate_field_forward_refs\u001b[1;34m(field, globalns, localns)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mtype_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m==\u001b[39m ForwardRef:\n\u001b[0;32m    519\u001b[0m     prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     field\u001b[38;5;241m.\u001b[39mtype_ \u001b[38;5;241m=\u001b[39m evaluate_forwardref(field\u001b[38;5;241m.\u001b[39mtype_, globalns, localns \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mouter_type_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m==\u001b[39m ForwardRef:\n\u001b[0;32m    522\u001b[0m     prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\typing.py:66\u001b[0m, in \u001b[0;36mevaluate_forwardref\u001b[1;34m(type_, globalns, localns)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_forwardref\u001b[39m(type_: ForwardRef, globalns: Any, localns: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Even though it is the right signature for python 3.9, mypy complains with\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# `error: Too many arguments for \"_evaluate\" of \"ForwardRef\"` hence the cast...\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Any, type_)\u001b[38;5;241m.\u001b[39m_evaluate(globalns, localns, \u001b[38;5;28mset\u001b[39m())\n",
      "\u001b[1;31mTypeError\u001b[0m: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer , SnowballStemmer , WordNetLemmatizer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer ,  TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6b9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Embedding, LSTM, SimpleRNN\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09779408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the datasets for nlp \n",
    "\n",
    "# nltk.download('movie_reviews')\n",
    "# nltk.download('treebank')\n",
    "# nltk.download('brown')\n",
    "# nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b7024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural Language Processing is a subfield of artificial intelligence that focuses on the interaction between humans and computers using natural language.\"\n",
    "text1 = \"I love natural language processing! It's so interesting and useful.\"\n",
    "text3 = \"Barack Obama was born in Hawaii and served as the 44th President of the United States from 2009 to 2017.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f3855",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = word_tokenize(text)\n",
    "print(token,end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbeb5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stp_word = set(stopwords.words('english'))\n",
    "print(stp_word,end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4050a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_token = [i for i in token if i.lower() not in stp_word ]\n",
    "print(filter_token,end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9bcb4-25c9-49ad-a2ff-4f2d4944f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f26a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamitizer = WordNetLemmatizer()\n",
    "\n",
    "lamitize_word = [lamitizer.lemmatize(i) for i in filter_token]\n",
    "print(lamitize_word,end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2831247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stem = PorterStemmer()\n",
    "porter_stem_word = [porter_stem.stem(i) for i in filter_token ]\n",
    "print(porter_stem_word,end='' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a135b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_ballss = SnowballStemmer('english')\n",
    "snow_ball_words = [snow_ballss.stem(i) for i in filter_token ]\n",
    "print(snow_ball_words,end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacyy = spacy.load('en_core_web_sm')\n",
    "\n",
    "spacy_word = [i.lemma_  for i in spacyy(' '.join(filter_token))]\n",
    "\n",
    "print(spacy_word,end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0803a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = CountVectorizer()\n",
    "x = vector.fit_transform([' '.join(filter_token)])\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883a0df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(filter_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d638a7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = TfidfVectorizer()\n",
    "Y = vectorizer2.fit_transform([text1])\n",
    "print(vectorizer2.get_feature_names_out())\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7916e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "746c75d7",
   "metadata": {},
   "source": [
    "# unit 1 , que 2. Implement a part-of-speech tagger using the NLTK library and evaluate its performance. Us the Penn Treebank dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434b250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "data = treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b3ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa555bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_l = list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e0381",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_l,end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ed833",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:3000]\n",
    "test_data = data[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67bb26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_tigg = nltk.DefaultTagger('NN')\n",
    "default_tigg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_tigg = nltk.UnigramTagger(train_data,backoff = default_tigg)\n",
    "print(uni_tigg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b85b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_tagger = nltk.BigramTagger(train_data, backoff=uni_tigg)\n",
    "bi_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dedcb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the tagger on the test data\n",
    "accuracy = bi_tagger.evaluate(test_data)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Tag a new sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tagged_sent = bi_tagger.tag(nltk.word_tokenize(sentence))\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45554bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf3c72f1",
   "metadata": {},
   "source": [
    "# unit 1\n",
    "\n",
    "# que 1. give me a code to Implement text preprocessing steps such as tokenization,\n",
    "# stopword removal, stemming, and\n",
    "# lemmatization. Use a subset of the NLTK's movie reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d40940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a134ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb09d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c977e16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews.fileids('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5531669",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [(movie_reviews.words(j), i ) for i in movie_reviews.categories() \n",
    "         for j in movie_reviews.fileids(i)[:50] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0764f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stem = PorterStemmer()\n",
    "lamitizer = WordNetLemmatizer()\n",
    "def preprocess_text(words):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(' '.join(words))\n",
    "    \n",
    "    # Convert to lower case\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Stopword removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmed_tokens = [porter_stem.stem(token) for token in tokens]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatized_tokens = [lamitizer.lemmatize(token) for token in stemmed_tokens]\n",
    "    \n",
    "    return lemmatized_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f704d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_documents = [(preprocess_text(words), category) for words, category in data1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e7ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71efb79a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeab31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5910eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit 2 \n",
    "\n",
    "# 1. give me a simple code Implement a tokenizer that splits a given text into sentences and words. Use a collection of news\n",
    "# articles from the Reuters-21578 dataset\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Load a collection of documents from the Reuters-21578 dataset\n",
    "documents = reuters.sents()  # Get sentences from the Reuters corpus\n",
    "print('docccc',documents)\n",
    "\n",
    "# Function to tokenize sentences and words\n",
    "def tokenize_reuters(documents):\n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    for sentence in documents:\n",
    "        # Join the sentence list into a single string\n",
    "        sentence_text = ' '.join(sentence)\n",
    "        print('sentences ',sentence_text)\n",
    "        \n",
    "        # Tokenize the sentence into words\n",
    "        words = word_tokenize(sentence_text)\n",
    "        print('word ',words)\n",
    "        \n",
    "        # Append the tokenized words and the original sentence\n",
    "        tokenized_sentences.append({\n",
    "            'original_sentence': sentence_text,\n",
    "            'tokenized_words': words\n",
    "        })\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "# Tokenize the first 5 documents for demonstration\n",
    "tokenized_data = tokenize_reuters(documents[:5])\n",
    "\n",
    "# Display the results\n",
    "for item in tokenized_data:\n",
    "    print(f\"Original Sentence: {item['original_sentence']}\")\n",
    "    print(f\"Tokenized Words: {item['tokenized_words']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec35b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b09fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. give me a simple code Implement a stemming algorithm (e.g., Porter Stemmer) and apply it to a text corpus.\n",
    "# Use a subset of\n",
    "# the Brown Corpus.\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Load a subset of the Brown Corpus (e.g., first 100 words from the 'news' category)\n",
    "subset = brown.words(categories='news')[:100]\n",
    "\n",
    "# Function to apply stemming to a list of words\n",
    "def apply_stemming(words):\n",
    "    stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "    return stemmed_words\n",
    "\n",
    "# Apply stemming to the subset of words\n",
    "stemmed_subset = apply_stemming(subset)\n",
    "\n",
    "# Display original and stemmed words\n",
    "print(\"Original Words: \", subset)\n",
    "print(\"\\nStemmed Words: \", stemmed_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2737bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a561ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. give me simple code Implement the Bag of Words model to convert a collection of text documents into numerical feature\n",
    "# vectors. Use the 20 Newsgroups dataset.\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the dataset to create the Bag of Words model\n",
    "X = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "# Convert the sparse matrix to a dense format and display the shape\n",
    "X_dense = X.toarray()\n",
    "print(\"Shape of the Bag of Words matrix:\", X_dense.shape)\n",
    "\n",
    "# Display the feature names (vocabulary)\n",
    "print(\"\\nVocabulary (features):\", vectorizer.get_feature_names_out()[:10])  # Display first 10 features\n",
    "\n",
    "# Display the first document's feature vector\n",
    "print(\"\\nFeature vector for the first document:\", X_dense[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b8956c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa66110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. give me simple code Implement TF-IDF vectorization and apply it to a text corpus to extract important features. Use the\n",
    "# IMDB movie reviews dataset.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load the IMDB movie reviews dataset\n",
    "imdb_data = fetch_openml('IMDb', version=1, as_frame=True)\n",
    "reviews = imdb_data.data['text']  # Extract the reviews\n",
    "labels = imdb_data.target  # Extract the labels\n",
    "\n",
    "# Create a TF-IDF Vectorizer instance\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the reviews to create the TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(reviews)\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame for better visualization\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the shape of the TF-IDF matrix\n",
    "print(\"Shape of the TF-IDF matrix:\", tfidf_df.shape)\n",
    "\n",
    "# Display the first few rows of the TF-IDF DataFrame\n",
    "print(\"\\nTF-IDF Matrix (first 5 rows):\")\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Display the first few terms with their corresponding TF-IDF values for the first review\n",
    "print(\"\\nTF-IDF values for the first review:\")\n",
    "print(tfidf_df.iloc[0].sort_values(ascending=False).head(10))  # Top 10 terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d723564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e59484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ccc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a82c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e5233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b56c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf988a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c211b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18221c91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081dd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bec7be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7c7a73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe999b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb7681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30434a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2afd4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6140ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c3d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8584757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c8bd46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
